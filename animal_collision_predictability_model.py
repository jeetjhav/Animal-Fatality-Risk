# -*- coding: utf-8 -*-
"""Animal Collision Predictability Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eHoUMgzHH4O5Tt7-ii0uktaqknxB9EJA
"""

import numpy as np
import pandas as pd
import random

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

"""**Implementation 1: Model that generates Risk scores based on entirely fake data. This version uses a function to replicate the "ground truth" on the risk score**

The output file will show the risk score similar to what would be fed into the web application
"""

#Fake dataset. The data will be fed into the model to create a predicted risk score
np.random.seed(42)

rows = 20000

data = pd.DataFrame({
    "hour": np.random.randint(0,24,rows),
    "month": np.random.randint(1,13,rows),
    "speed_limit": np.random.choice([25,35,45,55,65,75],rows),
    "traffic_volume": np.random.randint(50,20000,rows),
    "forest_density": np.random.uniform(0,1,rows),
    "distance_to_water_km": np.random.uniform(0,5,rows),
    "urban_density": np.random.uniform(0,1,rows),
    "past_collisions": np.random.poisson(2,rows),
    "temperature": np.random.uniform(-10,95,rows)
})

#Simulaate risks. This is the "ground truth" the model is reading from. This should not exist for when actual datasets are being used
def compute_risk(row):

    risk = 1

    # dusk & dawn danger
    if row.hour in [5,6,7,18,19,20]:
        risk += 3

    # fall migration (Oct-Nov)
    if row.month in [10,11]:
        risk += 2

    # forests = animals
    risk += row.forest_density * 4

    # near water = travel corridors
    if row.distance_to_water_km < 1:
        risk += 2

    # higher speed worse
    risk += row.speed_limit / 25

    # previous history
    risk += row.past_collisions * 0.8

    # urban safer
    risk -= row.urban_density * 3

    # heavy traffic moderate increase
    risk += np.log1p(row.traffic_volume)/2

    return max(1, min(10, risk))

data["risk_score"] = data.apply(compute_risk, axis=1)

#Model Training
X = data.drop(columns=["risk_score"])
y = data["risk_score"]

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)

model = RandomForestRegressor(n_estimators=120, max_depth=12)
model.fit(X_train,y_train)

pred = model.predict(X_test)

print("MAE:", mean_absolute_error(y_test,pred))

locations = pd.read_csv("/michigan_collision_risk_preview.csv")
#Michigan only filter

def random_features():
    return pd.DataFrame([{
        "hour": random.randint(0,23),
        "month": random.randint(1,12),
        "speed_limit": random.choice([35,45,55,65,75]),
        "traffic_volume": random.randint(200,15000),
        "forest_density": random.random(),
        "distance_to_water_km": random.uniform(0,3),
        "urban_density": random.random(),
        "past_collisions": random.randint(0,6),
        "temperature": random.uniform(20,80)
    }])

predictions = []

for i in range(len(locations)):
    features = random_features()
    risk = model.predict(features)[0]
    predictions.append(round(risk,1))

locations["risk_score"] = predictions
locations.to_csv("ml_generated_risk.csv",index=False)

print("Generated ml_generated_risk.csv")

"""**Implementation 2: of the Model with a real Deer Collision Dataset**

The folliwing code uses an existing data set of deer collisions across 23 states to determine a risk score. the model also reads a fake dataset on non collision events. The output file from this code will all be the same values as the city locations are all in Michigan with similar inputs on date/time/ month.

"""

#Access to actual dataset on deer collisions, but more data is needed on non collision events
df = pd.read_csv("/content/DVC_data.csv")

df = df[df["state_abbreviation"] == "MI"].copy()
df = df[[
    "date_time",
    "time.decimal",
    "month",
    "doy",
    "week",
    "sunrise.decimal",
    "sunset.decimal",
    "lat_CountyCentroid",
    "long_CountyCentroid"
]].dropna()

df["date_time"] = pd.to_datetime(df["date_time"])

df["hour"] = df["date_time"].dt.hour
df["is_night"] = (df["time.decimal"] < df["sunrise.decimal"]) | (df["time.decimal"] > df["sunset.decimal"])

# minutes from sunset (animals move right after sunset)
df["hours_from_sunset"] = abs(df["time.decimal"] - df["sunset.decimal"])

df["collision"] = 1

#Fake collision data generation:
import numpy as np

neg = df.sample(len(df)).copy()

neg["hour"] = np.random.randint(0,24,len(neg))
neg["time.decimal"] = np.random.uniform(0,24,len(neg))
neg["is_night"] = np.random.choice([0,1],len(neg))
neg["hours_from_sunset"] = np.random.uniform(0,12,len(neg))

neg["collision"] = 0

#Combine the dataset on non collisions (fake) and the exisitng dataset (real)
data = pd.concat([df,neg],ignore_index=True)

#Create a classification model to determine the liklihood of a deer collision to happen
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

features = [
    "hour",
    "month",
    "week",
    "is_night",
    "hours_from_sunset",
    "doy"
]

X = data[features]
y = data["collision"]

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

model = RandomForestClassifier(n_estimators=150,max_depth=12)
model.fit(X_train,y_train)

print("Accuracy:", model.score(X_test,y_test))

#Take the probablitlity score and convert it to a risk score
def prob_to_risk(p):
    return round(1 + 9*p,1)

locations = pd.read_csv("/michigan_collision_risk_preview.csv")

import datetime
now = datetime.datetime.now()

rows = []

for _,loc in locations.iterrows():

    sample = pd.DataFrame([{
        "hour": now.hour,
        "month": now.month,
        "week": now.isocalendar()[1],
        "is_night": int(now.hour < 7 or now.hour > 19),
        "hours_from_sunset": abs(now.hour - 19),
        "doy": now.timetuple().tm_yday
    }])

    p = model.predict_proba(sample)[0][1]
    rows.append(prob_to_risk(p))

locations["risk_score"] = rows
locations.to_csv("ml_generated_risk.csv",index=False)